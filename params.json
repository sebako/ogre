{"note":"Don't delete this file! It's used internally to help with page regeneration.","body":"---\r\nlayout: simple\r\ntitle: OGRE - OpenGrok Retrieval Engine\r\n---\r\n\r\nOGRE - OpenGrok Retrieval Engine\r\n================================\r\n\r\nOGRE is a Java library that allows programmatic access to [OpenGrok][1] servers.\r\nSince OpenGrok provides no web service API in the strict sense - it ouputs XHTML,\r\nand the project maintains that the format will not change or change infrequently -\r\nthis is effectively done by \"web scraping\", i.e. parsing the HTML output returned\r\nby the OpenGrok web server. The results are wrapped in Java objects and delivered\r\nto the caller.\r\n\r\n[1]: http://hub.opensolaris.org/bin/view/Project+opengrok/\r\n\r\n\r\nLibrary usage\r\n-------------\r\n\r\n**Notice:** This library can put a high, atypical load on servers that might be\r\ndimensioned for human readers browsing only a small part of the results. Certain\r\nmeasures are provided to reduce server load or allow servers to react\r\nappropriately, namely:\r\n\r\n *  the number of result pages that are browsed can be limited;\r\n *  pages providing more line matches than shown on the result pages are\r\n    also visited by default, but this can be switched off;\r\n *  the library sets a custom User-Agent header when contacting HTTP servers.\r\n\r\nNonetheless, users are discouraged from accessing a site with this library without\r\nthe consent of the administrator.\r\n\r\n\r\n### Basic search\r\n\r\nThe central starting point for a search run is the `Scraper` class. Here is a\r\nsimple example that shows how the class can be used:\r\n\r\n    URL baseURL = new URL(\"http://source.example.com/\");\r\n    Scraper scr = new Scraper(baseURL);\r\n    \r\n    String query = null, defs = null, path = null, hist = null, project = null;\r\n    String refs = \"str\";\r\n    \r\n    SearchResult result = scr.search(query, defs, refs, path, hist, project);\r\n    for (FileMatch fm : result.files())\r\n        System.out.println(fm.getFullName());\r\n\r\nIf you must use a web proxy to access the server, you can use an alternative\r\nconstructor:\r\n\r\n    Proxy proxy = new Proxy(Type.HTTP, new InetSocketAddress(\"proxy.example.com\", 8080));\r\n    Scraper scr = new Scraper(baseURL, proxy);\r\n\r\nThe basic model is that you initialise a Scraper instance with the basic URL\r\nof your OpenGrok server - that is to say, the URL that will bring up the\r\nsearch form - and then you can use this instance to place one or multiple\r\nsearch queries with the server.\r\n\r\nOpenGrok allows for five categories of search entries, plus project\r\nselection, and these are recognisable on most OpenGrok welcome pages as\r\nfive entry fields (the project is usually selected in a list box); however\r\nin OGRE we will usually refer to them by certain shorter informal names:\r\n\r\n 1. Full search (query)\r\n 2. Definition (defs)\r\n 3. Symbol (refs)\r\n 4. File path (path)\r\n 5. History (hist)\r\n 6. Project (project)\r\n\r\nThe `search()` method always takes these six parameters, but typically the\r\nuser will not want to fill in search terms for every one of these fields.\r\nFor unused fields, you can pass `null` or an empty or whitespace-only string.\r\nIn that case the field will not be included in the query sent to the server.\r\n\r\n\r\n### Understanding the search run\r\n\r\nThe Scraper class presents a simple enough interface, but it also offers a\r\nnumber of configuration options. In order to understand what these do, it is\r\nnecessary to get a picture of how `search()` performs searches.\r\n\r\nGenerally for a given combination of search terms, OpenGrok returns a list of\r\nfiles in the indexed source tree that match the query. The files are grouped\r\nby directory, and for every file there is a list of lines that match the query,\r\nthus making the file a match, although it is possible that a file contains\r\nno matching lines and is included in the results only because its path or a\r\nhistory entry matches.\r\nThe files are listed on a sequence of result pages that have to be retrieved\r\nindividually by the user; each page contains links to some, but not necessarily\r\nall preceding and subsequent pages.\r\n\r\nThe result pages also list the matching lines for each file, but this list may\r\nbe incomplete so as not to clutter the page in the case of files with many\r\nmatching lines. That case is indicted by an \"All\" link beneath the last listed\r\nline that leads to a further page containing all the lines with matches for\r\nthe file, given the search terms.\r\n\r\nIn order to retrieve all this information, the scraper must do the following:\r\n\r\n 1. Place an initial request to the server, resulting in the first of the\r\n    result pages.\r\n 2. Iterate over the result pages, recording the files.\r\n 3. For every file that includes a link to the complete line match list,\r\n    request that page and amend the line matches for that file.\r\n\r\nThe scraper can be configured to modify the order in which this happens, and\r\nrestrictions can be placed on the amount of data that is retrieved, as\r\nexplained in the following section.\r\n\r\n\r\n### Configuring the scraper instance\r\n\r\n#### Page limit\r\n\r\nThe number of result pages visited can be limited by calling `setPageLimit()`.\r\nThe scraper will start at the first result page and then visit the remaining\r\nones in order until the limit is reached. (OpenGrok supposedly orders search\r\nresults by \"relevance\".)\r\n\r\nThe following points should be noted:\r\n\r\n *  When a new scraper instance is created, it will have a default page limit\r\n    of 20.\r\n *  Setting a limit of 0 or less will mean that no HTTP request is sent to the\r\n    server at all. You may have set an invalid hostname and not even get an\r\n    exception in that case.\r\n *  The page limit cannot really be switched off, although setting the limit\r\n    to an extremely high number, such as `Integer.MAX_VALUE`, will have the\r\n    same effect.\r\n\r\nIf the page limit is triggered, this can be determined from the search result\r\nobject returned by the `search()` method:\r\n\r\n    if (result.pageLimitTriggered())\r\n        System.out.printf(\r\n                \"Page limit was triggered: \" +\r\n                \"Only %d pages were fetched, \" +\r\n                \"at least %d more would have been available.%n\",\r\n                result.fetchedPageCount(), result.unfetchedPageCount());\r\n\r\nAs long as there is at least one unfetched page, it is not certain how many\r\npages *really* went unfetched (i.e. would have been fetched by a full\r\nsearch), because the last page could have contained more links to further\r\nresult pages. This is indeed not a purely theoretical situation, as OpenGrok\r\nservers typically show, for example, links to pages 12 to 20 only from page\r\n11 on, not on page 10. This is why the number of unfetched pages that you\r\ncan read from the result object refers only to known unfetched pages, and\r\ntherefore only represents a minimum number of unfetched pages.\r\n\r\n\r\n#### Retrieval of additional lines\r\n\r\nBy default the scraper will visit links to the full line match lists for files\r\nwhose matching lines are not completely listed on the result page. These\r\nrequests may be responsible for a fair amount of the overall network traffic\r\ngenerated by a single search, but the user may actually be only interested in\r\nthe names of the matching files, or may for whatever reason be content with\r\nthe excerpts found on the result pages. Therefore the retrieval of additional\r\nlines matches can be switched on and off by calling `setFetchLines()` with the\r\nappropriate boolean value.\r\n\r\nIf it is switched off, and a file match has a link to the full line list, it\r\nis marked as abridged, so that the caller can determine whether the returned\r\nsearch results were affected by this setting. In addition, the result object\r\noffers the convenient method `abridgedFileCount()`.\r\n\r\n\r\n#### Order of retrieval\r\n\r\nIf the retrieval of addtional line matches is switched on, by default it is\r\ndeferred until after all result pages have been retrieved. This can be changed\r\nso that after every result page has been parsed, all line matches from that\r\npage are retrieved before the scraper continues with the next result page.\r\nThis is mostly of interest if the caller has configured the scraper to issue\r\nnotifications of interim results (see the next section).\r\n\r\nTo change the retrieval order, call `fetchLinesLast()` with the appropriate\r\nboolean value.\r\n\r\n\r\n### Notifications while the search is running\r\n\r\nThe search may run for a considerable time, depending on server and network\r\nspeed and result size. It is not unusual for it to last many seconds. The\r\nscraper offers to issue notifications of search progress before every\r\nHTTP request, and also delivery of partial search results. This way, an\r\napplication can show a progress bar or fill the search result display with\r\nnew file matches as they arrive.\r\n\r\nTo make use of this, an application has to register a progress or result\r\nlistener with its scraper instance *before* calling `search()`. The\r\nfollowing two sections describe how to use these listeners.\r\n\r\n\r\n#### Progress notifications\r\n\r\nThe progress listener has to methods:\r\n\r\n *  `progress()` is called before every request that is sent to the\r\n    OpenGrok server. It takes three arguments:\r\n    \r\n     1. Phase: The retrieval phase that the next request belongs to; this\r\n        means whether the scraper will request a new result page (phase\r\n        PAGES) or a list of line matches (phase LINES).\r\n     2. Running count of the request that is currently being prepared;\r\n     3. Minimum number of requests that are still pending (page and line\r\n        requests added up).\r\n   \r\n    The current and pending numbers can in principle be used to calculate\r\n    a progress percentage. But note that due to the nature of the search run,\r\n    the full number of requests is not known initially or during all\r\n    intermediate stages, because every new result page can bring links\r\n    to more result and line pages. The only fact that is certain is this:\r\n    If the scraper was configured with `fetchLinesLast(true)` and the phase\r\n    is LINES, then pending will not grow any more.\r\n\r\n *  `currentCounts()` is called after every request and informs the\r\n    listener about the updated counts of directories, files, and line\r\n    matches.\r\n\r\n\r\n#### Intermediate results\r\n\r\n(To be added.)","name":"Ogre","tagline":"OGRE: OpenGrok Retrieval Engine","google":""}